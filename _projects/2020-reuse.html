---
title: "Reuse-Centric Programming System Support of Machine Learning"
role: "09/2016 - present"
excerpt: "<br /><img src='/images/reuse-centric.png' alt='Reuse-Centric Optimization' style='width:500px;float:center'> <br />(Figure: Reuse-centric optimization.)<br /><br />As a critical link between software and computing hardware, programming system plays an essential role in ensuring the efficiency, scalability, security, and reliability of machine learning. This project examines the challenges in machine learning from the programming system perspective by developing simple yet effective reuse-centric approaches. Specifically, 
<ul class='archive__item-excerpt'>
	<li>Proposed a flexible ensemble DNN training framework for efficiently training a heterogeneous set of DNNs; achieved up to 1.97X speedups over the state-of-the-art framework that was designed for homogeneous DNN ensemble training. (Published in [<a href='http://guanh01.github.io/files/2020mlsys.pdf'>MLSys'20</a>])  </li>

	<li> Proposed in-place zero-space ECC assisted with a new training scheme, weight distribution-oriented training, to provide the first known zero space cost memory protection for CNNs. (Published in [ <a href='http://guanh01.github.io/files/2019nips.pdf'>NeurIPSâ€™19</a>])  </li>

	<li> Developed a compiler-based framework that, for the first time, enables composability-based CNN pruning by generalizing Teacher-Student Network training for pre-training common convolutional layers; achieved up to 186X speedups. (Published in [ <a href='http://guanh01.github.io/files/2019pldi.pdf'>PLDI'19</a>])  </li>

	<li>  Accelerated CNN training by identifying and adaptively avoiding similar vector dot products during training on the fly; saved up to 69% CNN training time with no accuracy loss. (Published in [<a href='http://guanh01.github.io/files/2019icde.pdf'>ICDE'19</a>])  </li>

	<li>  Improved the performance of DNN ensemble training by eliminating pipeline redundancies in preprocessing through data sharing; reduced CPU usage by 2-11X. (Published in [<a href='http://guanh01.github.io/files/2018sc.pdf'>SC'18</a>])  </li>

	<li>  Accelerated K-Means configuration by promoting multi-level computation reuse across the explorations of different configurations; achieved 5-9X speedups. (Published in [<a href='http://guanh01.github.io/files/2018icde.pdf'>ICDE'18</a>])  </li>

	<li>  Accelerated distance calculation-based machine learning algorithms (K-Means, KNN, etc.) by developing Triangle Inequality-based strength reduction; produced tens of times of speedups. (Published in [<a href='http://guanh01.github.io/files/2017pldi.pdf'>PLDI'17</a>])  </li>

</ul>

"
collection: projects
---
